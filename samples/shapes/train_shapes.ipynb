{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\Lars M. Angelsen\\MaskRCNN\\logs\\shapes20190910T1144\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Lars M. Angelsen\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1/convolution}}]]\n\t [[ROI/GatherV2_2/_4467]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1/convolution}}]]\n0 successful operations.\n0 derived errors ignored.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bc1cfcf80450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             layers='heads')\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2374\u001b[1;33m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2375\u001b[0m         )\n\u001b[0;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1658\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lars m. angelsen\\.conda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1/convolution}}]]\n\t [[ROI/GatherV2_2/_4467]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1/convolution}}]]\n0 successful operations.\n0 derived errors ignored."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=100, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
